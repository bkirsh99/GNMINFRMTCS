---
title: "BMEG 400E: Assignment 2"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment Overview

The goal of this assignment is to walk you through the process of creating a pipeline. For this, we will be analyzing the data from the same research article as last time ([*https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7772509/*](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7772509/){.uri}) by Choi, Won-Young, et al. (2020). This research group was looking at how different epigenetic marks changed in an induced Pluripotent Stem Cell (iPSC) when it was differentiated towards a Neuronal Progenitor Cell (NPC). For this assignment, we will be looking at one histone mark (H3K27me3) and its behavior in iPSCs. This data was created by looking at Chromatin Immunoprecipitation sequencing data (ChIP-seq) using an antibody against the H3K27me3 to sequence only the DNA that attached to the epigenetic mark. Then, we can use this data to find out where H3K27me3 marks are located in the genome by mapping the ChIP-seq reads to the genome. However, to make sure we are seeing a true enrichment of a region, we need a control to compare to. This control is called the *input*, which is usually essentially the same proceedure as was applied to the ChIP-seq sample, but with the immunoprecipitation step skipped. By comparing the input to the ChIP data, we can distinguish between noise and true enrichements. For this assignment, we will be finding how H3K27me3 changes when iPSCs undergo differentiation. All fastq files are paired-end and can be found under the following path:**/usr/local/share/data/assignment_2/**. As always, remember not to copy these files to your directory or try to alter them in any way!

For iPSC:

-   Input for iPSC: *input_iPSC_SRA66_subset_1.fastq.gz* and *input_iPSC_SRA66_subset_2.fastq.gz*

-   H3K27me3 for iPSC: *H3K27me3_iPSC_SRA60_subset_1.fastq.gz* and *H3K27me3_iPSC_SRA60_subset_2.fastq.gz*

We will not be using the NPC data in this assignment.

**This assignment follows a simplified version of a ChIP-seq analysis prior to the peak callling using the following steps:**

a.  Analyze the reads quality: fastqc ([*https://www.bioinformatics.babraham.ac.uk/projects/fastqc/*](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/){.uri})

b.  Mapping the reads to the genome: bowtie2 ([*http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml*](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml){.uri})

c.  Convert sam to bam files: samtools (\*\*)

d.  Sort the alignment files by genomic position: sambamba ([*https://lomereiter.github.io/sambamba/docs/sambamba-view.html*](https://lomereiter.github.io/sambamba/docs/sambamba-view.html){.uri})

e.  Filter for uniquely mapped reads: sambamba

## Before we get started

Before we get started we will assume that you have successfully created a Github repository that will host all your future assignments. For this assignment make sure to create a new file under the same repository so that you don't need to make a new one. You can just make a new subdirectory. At the end, all you should need to do is the standard "commit, pull, push" actions when you are done the assignmenty. If you make a new one, rememnber to keep it private and add this accound as a collaborator.

All the questions that need to be answered with a command or a written response, from now on, will be marked with **\#?\#** and be followed by the grading of the question.

Remember to read carefully each step and to use the tips only as tips and should not be used as instructions on how to do things! You are welcome to use other tools and commands you feel more comfortable with **as long as you explain your thought process and show that you reached the same goal**. Also remember to pay attention to the error messages and try to understand what is it saying, Google it and try a few things by yourself! Most of the times looking at the tools documentation would solve it. As a final note, these tools can be daunting, so don't go into any rabbit holes, less is more, you won't need to modify any of the default parameters, just make sure you are getting using the right input and output files. Okay, let's start!

**This assignment has 3 main goals:**

1.  To go through *steps a-e* for the **input data**

2.  To create a pipeline that does a-e automatically for the **H3K27me3 data**

## 0. Getting the right tools

As you might have seen on the overview we will be using lots of new tools today. So before jumping into the analysis, let's make sure we install them all. Luckily for us, we are all familiar with how to install things using conda (see your assignment 1)!

```{bash, eval=FALSE}

#?# Add sambamba to your conda environment created on assignment 1 - 1pt

conda install -n test_env -c bioconda sambamba

## Note: to complete this assignment, it is expected that your conda environment already has installed: fastqc, bowtie2 and samtools 


```

## 1. Analyzing ChIP-seq data (steps a-e)

For this first part of the assignment, we will work **only with the iPSC input data.**

### a. Quality Controls

Similarly to the last assignment, we need to make sure that the data has a good quality before we can move on to further steps. The files that we are working with are paired-end, meaning that there are two reads per each sequence, one starting on the 5' (\_1) and the other on the 3' (\_2). Using the **fastqc** tool that we reviewed last time, do a quick quality check of the two files included in your set (\_1 and \_2).

```{bash, eval=FALSE}

## NOTE: Remember to use screen activate your conda environment!

screen -S background_conda
conda activate test_env

#?# Type below the command that you used to perform the fastqc analysis on the paired-end iPSC input files: - 0.5 pt

mkdir -p assignment2/fastqc_output/read1
fastqc /usr/local/share/data/assignment_2/input_iPSC_SRA66_subset_1.fastq.gz -o assignment2/fastqc_output/read1

mkdir -p assignment2/fastqc_output/read2
fastqc /usr/local/share/data/assignment_2/input_iPSC_SRA66_subset_2.fastq.gz -o assignment2/fastqc_output/read2

## Copy the html output file to your computer, open it and write below what can you tell about the data quality of these files.
## You can reference fastqc documentation if in doubt.

# ANSWER: On a new window, I ran the following commands from my local machine.

scp bkirsh@137.82.55.186:/home/bkirsh/assignment2/fastqc_output/read1/input_iPSC_SRA66_subset_1_fastqc.html /mnt/c/Users/bkirs/Documents

scp bkirsh@137.82.55.186:/home/bkirsh/assignment2/fastqc_output/read2/input_iPSC_SRA66_subset_2_fastqc.html /mnt/c/Users/bkirs/Documents

# ANSWER: The quality of both files is exceptionally high all throughout. The drop over the length of the read is expected, but minor. All together, the mean quality score at each base position is consistently elevated with minimal variability (blue line). While both reads have a generally high quality score, read 2 is noticeably lower quality than read 1, especialy towards the end.
  
#?# Are there any differences between the files for read 1 and read 2? Describe what you see below: - 1.5 pt
## -
## -

# ANSWER: In both files, the basic statistics such as total number of reads, reads flagged as poor quality, read length, and GC content are the same. However, the average quality scores for read 1 (~1600000) is higher than for read 2 (~1200000). This is expected with paired-end reads and is more clearly attested by the "per base sequence quality" and "per sequence quality scores" plots. In general, Illumina sequencing produces reads whose quality decreases towards the end, with read 2 being often worse quality than read 1. Thus, the lowest quality bases are at the end of read 2.

## NOTE: Same as last time, in order to open the html files on your web browser you will need to download the files to your computer
## Ubuntu/Linux and Mac users: look at the *scp* command
## Windows users: you can follow these instructions: https://stackoverflow.com/questions/6217055/how-can-i-copy-a-file-from-a-remote-server-to-using-putty-in-windows

```

### b. Mapping to the reference genome

```{bash, eval=FALSE}

#?# Perform a paired-end alignment of the iPSC input fastq sequences to the human genome using bowtie2 - 1.5 pt
## Use the previously created index located in: /usr/local/share/indexes/hg38_bowtie2_index 

bowtie2 -x /usr/local/share/indexes/hg38_bowtie2_index -1 /usr/local/share/data/assignment_2/input_iPSC_SRA66_subset_1.fastq.gz -2 /usr/local/share/data/assignment_2/input_iPSC_SRA66_subset_2.fastq.gz -S assignment2/bowtie2_output.sam

## Tip: look at the bowtie2 --help or the documentation on the tool website (http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#getting-started-with-bowtie-2-lambda-phage-example)

```

### c. Sam to Bam file convertion

Next we must convert the sam files made by bowtie2 to bam files, which are much smaller in size and faster to access.

```{bash, eval=FALSE}
#?# Using *samtools view*, convert the sam file containing the result of the alignment of the iPSC input files to the reference genome (last step output) to bam format - 1.5 pt
## Don't forget to include the header! (-h flag)

samtools view -h -b -o assignment2/bowtie2_output.bam assignment2/bowtie2_output.sam

```

### d. Bam indexing and sorting

BAM files are much smaller and faster to access than SAM files, but unless the reads are sorted by chromosome and position, we can't do much with it since the reads relevant for one locus would be scattered randomly throughout in whatever order they had been aligned. Further, indexing is important to allow programs like IGV to access the BAM file. Indexing is essentially adding little bookmarks throughout the file so that a program accessing the BAM knows where to find data relevant to whatever locus it is interested in. The `sambamba` tool can be used to sort bam files and create an index simultaneously. You may need to install it via anaconda.

```{bash, eval=FALSE}

## Run the following command, changing the step_d_output.bam for the name of the bam output file you got from step c; this will print the read ID, chromosome, and genomic position for each read alignment; replace <step_c_output>.bam with the name of your bam file from the last step.

samtools view assignment2/bowtie2_output.bam | cut -f1,3,4 | head -5

#?# Use the documentation format (# at the beginning of the line) to include the output of the command above (one per line): - 0.5pt

# SRR12694366.1000000     chr3    46631700
# SRR12694366.1000000     chr3    46631857
# SRR12694366.10000029    chr3    77719001
# SRR12694366.10000029    chr3    77718984
# SRR12694366.10000035    chr3    40015576

## Using *sambamba sort*, sort the bam file that you created on the last step
## sambamba sort default will sort the file and build an index that will allow us to look at the reads that mapped to a specific positions in the genome
#?# Type the command you use below: - 1 pt

sambamba sort assignment2/bowtie2_output.bam

## View the read ID, chromosome, and genomic position for the first 5 reads, as before, but this time for the sorted bam file you just made.

samtools view assignment2/bowtie2_output.sorted.bam | cut -f1,3,4 | head -5

#?# Use the documentation format (# at the beginning of the line) to include the output of the command above (one per line): 0.5 pt

# SRR12694366.7794671     chr12   133198776
# SRR12694366.7794671     chr12   133198776
# SRR12694366.26500791    chr3    9988
# SRR12694366.33624963    chr3    10013
# SRR12694366.33624963    chr3    10013

#?# What changed between the two files? Describe what is sambamba sort doing. - 1 pt
## If needed, you can inspect more of each bam file by increasing the -n parameter for `head` in the above (just don't bother to include more than 5 lines for each in your submission).

# ANSWER: The first file is unsorted, while the second file is sorted by reference id and start coordinate. This can be further attested by running "samtools view -h <file.bam> | head," which outputs "@HD     VN:1.0  SO:unsorted" and "@HD     VN:1.0  SO:coordinate" for the first and second files, respectively. As such, sambamba sorts primarily by the RNAME field (i.e. reference id), whose order is determined by the order of @SQ lines in the header. For alignments with the same RNAME, reads are ordered by increasing start coordinate (i.e., POS field) and for alignments with the same RNAME and POS, order is arbitrary. The result of sambamba sorting is a file where alignments are grouped by both RNAME and POS.


```

### e. Keep only uniquely mapped reads

Next, we want to create a version of the BAM file that contains only uniquely mapping reads. We will be using these data to call peaks and want to know which are different between cell types. The reads that do not map uniquely are stochastically assigned to one of the several best possible matches. This could lead to regions that have more reads in one cell type vs the other by chance alone. Accordingly, we want to remove these ambiguously mapping reads.

```{bash, eval=FALSE}

#?# Use *sambamba view* to filter the recently sorted bam file (previous step output) to include only uniquely mapped reads - 1 pt
## For this we will make use of the *-F* flag to filter the reads that were mapped and are not duplicates, by adding the following flag:
## *  -F "[XS] == null and not unmapped and not duplicate"  *
## Important: Remember to add the header (-h flag) and to specify the output file format (-f flag) as bam

sambamba view -F "[XS] == null and not unmapped and not duplicate" -h -f bam -o assignment2/bowtie2_output.filtered.bam assignment2/bowtie2_output.sorted.bam

#?# How many reads were there before filtering for uniquely mapped reads? How many are there now? Include the code to answer these questions and the answers to them below.

samtools view -c assignment2/bowtie2_output.sorted.bam

# ANSWER: The output (reads before filtering) is: 4444490

samtools view -c assignment2/bowtie2_output.filtered.bam

# ANSWER: The output (reads after filtering) is: 3859882
```

Now that you have created your BAM files and inspected them, now would be a good time to delete the SAM files that are leftover from your alignment.

The next step in the process would be calling peaks, but we need both input and ChIP data for each cell type before we can do that and here we have only processed one file. So instead, we will be...

## 2. Implementing a pipeline

We have gone through all these steps for this first file, the input iPSC file. Now we want to make a pipeline that could be used for all four files where we have abstracted away the sample IDs, and instead have code that will work on any given sample name.

In this section, you will need to edit files on the command line. There are several ways in which you can do this. The simplest text editor is `pico`, with `emacs` being more intermediate, and `vim` being powerful, but difficult to learn. You can find tutorials for any of these using google. It would be best for you to learn one of these, rather than constantly moving files back and forth to/from your computer and the server.

### a. Make a task list

Start by making a tab delimited file with three columns: sampleID, fastqFile1, fastqFile2 A sample first line is included here:

    iPSC_input  input_iPSC_SRA66_1.fastq.gz  input_iPSC_SRA66_2.fastq.gz

This will be the list of tasks you want to accomplish.

### b. Working with a "job scheduler"

Normally we would use a job scheduler to accomplish these tasks, but our server is incapable of using a job scheduler, so instead we're going to use this program (<https://raw.githubusercontent.com/BMEGGenoInfo/Assignments/main/Assignment_2/runTheseJobsSerially.sh>) that will run a given script for each line in a file. Download it to the server, for instance using `wget`.

Now run the following command, where you should replace <taskfile> with the name of your file. Note that you may need to run `chmod +x runTheseJobsSerially.sh` to give the script file execute permissions.

```{bash, eval=FALSE}
./runTheseJobsSerially.sh echo tasks.txt
#?# what happened? Enter your answer below - 1 pt

# ANSWER: The contents of the file were printed to STDOUT.
```

### c. Your first pipeline

Now we want to make a script that will run all the commands you performed above (in Q1), but for any of the samples. Below is a script which you can copy into a new file "fastqToFilteredBam.sh", which you will modify to include all the steps needed to go from a fastq to a sorted, filtered (uniquely mapping only) bam file.

```{bash, eval=FALSE}
#!/bin/bash
set -e # this makes the whole script exit on any error.
#fill these variables with the arguments given to this script
sample=$1
fq1=$2
fq2=$3
logDir=MyLogDirectory # this is where all the files to keep track of progress will go.
mkdir -p MyLogDirectory # make the directory where log files will go, if it doesn't exist already
echo running pipeline for $sample
if [ ! -e $logDir/$sample.fastqc.done ] #run this code only if $logDir/$sample.fastqc.done is missing
then
        echo Performing fastqc of sample $sample with the following fastqs:
        ls /usr/local/share/data/assignment_2/$fq1 /usr/local/share/data/assignment_2/$fq2
        
        #enter commands to run fastqc here
        
        touch $logDir/$sample.fastqc.done #create the file that we were looking for at the beginning of this if statement so that this same code is not run next time
else # $logDir/$sample.fastqc.done was not missing
        echo Already performed fastqc of $sample
fi
#here is where you will be including the additional steps to take you from fastq.gz to sorted BAM containing only uniquely mapping reads.
```

Now run the following:

```{bash, eval=FALSE}
## Run this with <taskfile> replaced with the name of your task file.
./runTheseJobsSerially.sh ./fastqToFilteredBam.sh tasks.txt
#?# What happened? Enter your answer below - 1 pt

# ANSWER: The following message is printed to STDOUT: "running pipeline for iPSC_input. Performing fastqc of sample iPSC_input with the following fastqs: /usr/local/share/data/assignment_2/input_iPSC_SRA66_subset_1.fastq.gz /usr/local/share/data/assignment_2/input_iPSC_SRA66_subset_2.fastq.gz." Furthermore, the "ls" command shows that a new directory has been created (i.e., MyLogDirectory), and that it contains an empty file called "iPSC_input.fastqc.done."

#?# Now run that same command again. What happened this time? Why? Answer below.  - 2 pts

# ANSWER: The following message is printed to STDOUT: "running pipeline for iPSC_input. Already performed fastqc of iPSC_input." This difference in behaviour is attributed to the "MyLogDirectory," which is responsible for keeping track of the tasks completed. At this point, the task file only contains a single line that corresponds to the task of converting the iPSC fastq into a sorted and filtered bam file. However, this task was already performed by running the previous command and a "flag" represented by the empty file handle in the MyLogDirectory ("iPSC_input.fastqc.done") was created. Thus, the task is not performed again.

#?# What can you do to make it run the same way as it did the first time? Answer below with the command(s) that would make it run the same way as the first time - 2 pts

# ANSWER: We can remove the "MyLogDirectory" (1), which keeps track of the all progress, or simply remove "MyLogDirectory/iPSC_input.fastqc.done" (2) to "unflag" that FastQC was already done for iPSC_input. Option (2) is preferable when the task file contains multiple tasks, since it allows the user to re-do specific steps rather than the whole pipeline. The commands for options (1) and (2) are the following:

rm -r MyLogDirectory
rm MyLogDirectory/iPSC_input.fastqc.done

```

### d. Filling in the pipeline

Now fill in the rest of the pipeline with the tasks you had done for iPSC input, but replace the sample IDs with the variable `$sample`. Include appropriate documentation and messages. Note that it is easier to separate the different parts of a file name with a period (.). For instance, bash will treat the `$sample` in `$sample.sorted.uniqMapping.bam` as the variable `$sample`, whereas `$sample_sorted_uniqMapping.bam` will be interpreted as a new varaible `$sample_sorted_uniqMapping`.

Note that in the script provided, fastqc is not actually run, so you will have to include the code for that too. Everything you needed to do to take the samples from fastq to sorted, uniquely mapping BAM file should be included in this pipeline. You should also build in robustness to failure, comments, and messages (e.g. via `echo`) to describe in the purpose of the code, and to provide updates while the code is running. You should test this script by running it on the two samples (iPSC input, and iPSC ChIP).

```{bash, eval=FALSE}
## Test the script by running this, with your modified pipeline script in place of fastqToFilteredBam.sh
./runTheseJobsSerially.sh ./fastqToFilteredBam.sh tasks.txt
```

When you are done, enter the code for your pipeline in the space provided below - 10 pts

```{bash, eval=FALSE}
## REPLACE THIS WITH YOUR SCRIPT
#!/bin/bash
#
# AUTHOR: Bianca Kirsh <bkirsh.3006@gmail.com>
# DATE: January 26, 2021
# FILE: fastqToFilteredBam.sh
#
# DESCRIPTION: This script pre-processes paired-end raw sequence data provided in FASTQ format to produce
# analysis-ready BAM files containing only uniquely mapped reads. This includes quality control using
# FastQC, alignment to a reference genome using Bowtie 2, and finally conversion of the SAM file into a
# sorted and filtered BAM file using Samtools and Sambamba.
#
# REQUIREMENTS: This scripts requies a CWL account and a connection to the course server (137.82.55.186)
# while logged into the UBC Virtual Private Network (VPN). Furthermore, it must be called from inside
# an active conda environment with the following packages installed:
#       - fastqc (https://anaconda.org/bioconda/fastqc)
#       - bowtie2 (https://anaconda.org/bioconda/bowtie2)
#       - samtools (https://anaconda.org/bioconda/samtools)
#       - sambamba (https://anaconda.org/bioconda/sambamba)
#
# USAGE: To run this script, simply enter the following command prompt in a unix shell:
#
#       ./fastqToFilteredBam.sh <sampleID> <fastqFile1> <fastqFile2>
#
# Alternatively, it can be passed as a command-line argument to the program https://raw.githubusercontent.com/BMEGGenoInfo/Assignments/main/Assignment_2/runTheseJobsSerially.sh.
#
#       ./runTheseJobsSerially.sh ./fastqToFilteredBam.sh <taskfile>
#
# In this case, the <taskfile> consists of a tab-delimited file with three columns: sampleID, fastqFile1,
# and fastqFile2. A sample first line is illustrated below.
#
# iPSC_input  input_iPSC_SRA66_subset_1.fastq.gz  input_iPSC_SRA66_subset_2.fastq.gz
#
# SPECIAL NOTES: For the purposes of this assignment, the workflow is designed to operate using pre-downloaded
# fastq files (/usr/local/share/data/assignment_2) and a pre-indexed human genome (/usr/local/share/indexes/hg38_bowtie2_index).
#

set -Eeuo pipefail # Abort the script on errors and unbound variables.

# Variables to hold the arguments given to this script
sample=$1
fq1=$2
fq2=$3
# Variables to indicate input and output file path locations
logDir=MyLogDirectory # this is where all the files to keep track of progress will go.
filePath=/usr/local/share/data/assignment_2 # this is the path to the pre-downloaded fastq files.
genome=/usr/local/share/indexes/hg38_bowtie2_index # this is the path to the pre-indexed genome file.
outDir=MyOutputDirectory/$sample # this is where the output files from the workflow will go.

mkdir -p MyLogDirectory # make the directory where log files will go, if it doesn't exist already
mkdir -p $outDir # make the directory where output files will go, if it doesn't exist already

echo Running pipeline for $sample

if [ ! -e $logDir/$sample.fastqc.done ] #run this code only if $logDir/$sample.fastqc.done is missing
then
        echo Performing fastqc of sample $sample with the following fastqs:
        ls $filePath/$fq1 $filePath/$fq2
        # Run a FastQC analysis on each of the input files.
        for file in $fq1 $fq2; do
                fastqc $filePath/$file -o $outDir
        done
        touch $logDir/$sample.fastqc.done #create the file that we were looking for at the beginning of this if statement so that this same code is not run next time
else # $logDir/$sample.fastqc.done was not missing
        echo Already performed fastqc of $sample
fi

if [ ! -e $logDir/$sample.sam.done ] #run this code only if $logDir/$sample.sam.done is missing
then
        echo Performing alignment of sample $sample with the following indexed genome file: $genome
        bowtie2 -x $genome -1 $filePath/$fq1 -2 $filePath/$fq2 -S $outDir/$sample.sam
        touch $logDir/$sample.sam.done #create the file that we were looking for at the beginning of this if statement so that this same code is not run next time
else # $logDir/$sample.sam.done was not missing
        echo Already performed alignment of $sample
fi

if [ ! -e $logDir/$sample.bam.done ] #run this code only if $logDir/$sample.bam.done is missing
then
        echo Performing SAM to BAM conversion of sample $sample
        samtools view -h -b -o $outDir/$sample.bam $outDir/$sample.sam
        touch $logDir/$sample.bam.done #create the file that we were looking for at the beginning of this if statement so that this same code is not run next time
else # $logDir/$sample.bam.done was not missing
        echo Already performed SAM to BAM conversion of $sample
fi

if [ ! -e $logDir/$sample.sorted.done ] #run this code only if $logDir/$sample.sorted.done is missing
then
        echo Sorting the BAM file $sample.bam
        sambamba sort $outDir/$sample.bam
        touch $logDir/$sample.sorted.done #create the file that we were looking for at the beginning of this if statement so that this same code is not run next time
else # $logDir/$sample.sorted.done was not missing
        echo Already sorted the BAM file for $sample
fi

if [ ! -e $logDir/$sample.filtered.done ] #run this code only if $logDir/$sample.filtered.done is missing
then
        echo Filtering BAM file $sample.bam to remove ambiguously mapped reads.
        sambamba view -F "[XS] == null and not unmapped and not duplicate" -h -f bam -o $outDir/$sample.filtered.bam $outDir/$sample.sorted.bam
        touch $logDir/$sample.filtered.done #create the file that we were looking for at the beginning of this if statement so that this same code is not run next time
else # $logDir/$sample.filtered.done was not missing
        echo Already filtered the BAM file for $sample
fi

echo Pre-processing complete for sample $sample. The output files are located in: $outDir
```
